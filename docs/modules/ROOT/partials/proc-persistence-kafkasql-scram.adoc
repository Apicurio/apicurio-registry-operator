[id="registry-persistence-kafkasql-scram"]
= Configuring Kafka storage with SCRAM security

You can configure the {kafka-streams} Operator and {operator} to use Salted Challenge Response Authentication Mechanism (SCRAM-SHA-512) for the Kafka cluster.


.Prerequisites

* You must install the {operator} using the OperatorHub or command line.
* You must install the {kafka-streams} Operator or have Kafka accessible from your OpenShift cluster.

NOTE: This section assumes that {kafka-streams} Operator is available, however you can use any Kafka deployment.
In that case, you must manually create the Openshift secrets that the {operator} expects.

.Procedure 

. In the OpenShift web console, click *Installed Operators*, select the *{kafka-streams}* Operator details, and then the *Kafka* tab. 

. Click *Create Kafka* to provision a new Kafka cluster for {registry} storage. 

. Configure the `authorization` and `tls` fields to use SCRAM-SHA-512 authentication for the Kafka cluster, for example:
+
[source,yaml]
----
apiVersion: kafka.strimzi.io/v1beta1
kind: Kafka
metadata:
  name: my-cluster
  namespace: registry-example-streams-tls
spec:
  kafka:
    authorization:
      type: simple
    version: 2.5.0
    replicas: 3
    listeners:
      plain: {}
      tls:
        authentication:
          type: scram-sha-512
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
      log.message.format.version: '2.5'
    storage:
      type: ephemeral
  zookeeper:
    replicas: 3
    storage:
      type: ephemeral
  entityOperator:
    topicOperator: {}
    userOperator: {}
----

. The default Kafka topic name that {registry} uses to store data is `kafkasql-journal`.
This topic is created automatically by {registry}.
You can override this behavior or the default topic name by setting the appropriate environment variables (default values):
** `REGISTRY_KAFKASQL_TOPIC_AUTO_CREATE=true` and
** `REGISTRY_KAFKASQL_TOPIC=kafkasql-journal`.

+
If you decide not to create the Kafka topic manually, skip the next step.

. Click the *Kafka Topic* tab, and then *Create Kafka Topic* to create the `kafkasql-journal` topic:
+
[source,yaml]
----
apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaTopic
metadata:
  name: kafkasql-journal
  labels:
    strimzi.io/cluster: my-cluster
  namespace: registry-example-streams-plain
spec:
  partitions: 2
  replicas: 1
  config:
    retention.ms: 604800000
    segment.bytes: 1073741824
----

. Create a *Kafka User* resource to configure SCRAM authentication and authorization for the {registry} user. For example, in the `spec` block, see the `authentication` section.
+
[source,yaml]
----
spec:
  authentication:
    type: scram-sha-512
  authorization:
    acls:
      - operation: All
        resource:
          name: '*'
          patternType: literal
          type: topic
      - operation: All
        resource:
          name: '*'
          patternType: literal
          type: cluster
      - operation: All
        resource:
          name: '*'
          patternType: literal
          type: transactionalId
      - operation: All
        resource:
          name: '*'
          patternType: literal
          type: group
    type: simple
----

. Click *Workloads* and then *Secrets* to find two secrets that {kafka-streams} creates for {registry} to connect to the Kafka cluster:
+
* `my-cluster-cluster-ca-cert` - contains the PKCS12 truststore for the Kafka cluster
* `my-user` - contains the user's keystore
+
NOTE: The name of the secret can vary based on your cluster or user name.

. If you create the secrets manually, they must contain the following key-value pairs:
+
* *my-cluster-ca-cert*
** `ca.p12` - the truststore in PKCS12 format
** `ca.password` - truststore password
* *my-user*
** `password` - user password

. Configure the following example settings to deploy the {registry}:
+
[source,yaml]
----
include::example$apicurioregistry_kafkasql_scram_cr.yaml[]
----

IMPORTANT: You must use a different `bootstrapServers` address than in the plain insecure use case. The address must support TLS connections, and is found in the specified *Kafka* resource under the `type: tls` field.
